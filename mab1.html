
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://rishacha.github.io/theme/stylesheet/style.min.css">

    <link id="dark-theme-style" rel="stylesheet" type="text/css"
          media="(prefers-color-scheme: dark)"
    href="https://rishacha.github.io/theme/stylesheet/dark-theme.min.css">

    <link id="pygments-dark-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: dark)"
          href="https://rishacha.github.io/theme/pygments/monokai.min.css">
    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"
          href="https://rishacha.github.io/theme/pygments/github.min.css">


  <link rel="stylesheet" type="text/css" href="https://rishacha.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://rishacha.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://rishacha.github.io/theme/font-awesome/css/solid.css">


    <link href="https://rishacha.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Rishacha's Blog Atom">


    <link rel="shortcut icon" href="/media/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/media/favicon.ico" type="image/x-icon">


    <!-- Chrome, Firefox OS and Opera -->
    <meta name="theme-color" content="#333333">
    <!-- Windows Phone -->
    <meta name="msapplication-navbutton-color" content="#333333">
    <!-- iOS Safari -->
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Microsoft EDGE -->
    <meta name="msapplication-TileColor" content="#333333">

<meta name="author" content="Rishabh Chakrabarti" />
<meta name="description" content="My cheat sheet for bandit algorithms. Ref. Lattimore and Szepesvari" />
<meta name="keywords" content="multi-armed-bandits">


<meta property="og:site_name" content="Rishacha's Blog"/>
<meta property="og:title" content="Multi-armed bandits and algorithms"/>
<meta property="og:description" content="My cheat sheet for bandit algorithms. Ref. Lattimore and Szepesvari"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://rishacha.github.io/mab1.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2022-05-22 00:00:00+05:30"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://rishacha.github.io/author/rishabh-chakrabarti.html">
<meta property="article:section" content="Online Learning and Optimization"/>
<meta property="article:tag" content="multi-armed-bandits"/>
<meta property="og:image" content="/media/rctest.jpg">

  <title>Rishacha's Blog &ndash; Multi-armed bandits and algorithms</title>

</head>
<body >
  <aside>
    <div>
   
      <a href="https://rishacha.github.io">
        <img src="/media/rctest.jpg" alt="Rishabh Chakrabarti" title="Rishabh Chakrabarti">
      </a>

      <h1>
        <a href="https://rishacha.github.io">Rishabh Chakrabarti</a>
      </h1>
      

<p>Developer searching for excitement</p>


    </div>

  </aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="mab1">Multi-armed bandits and algorithms</h1>
    <p>
      Posted on May 22, 2022 in <a href="https://rishacha.github.io/category/online-learning-and-optimization.html">Online Learning and Optimization</a>

    </p>
  </header>


  <div>
    <h3>Revision for <span class="math">\(k\)</span>-armed bandit algorithms and problem setting</h3>
<table>
<thead>
<tr>
<th>Action space</th>
<th>Problem Structure</th>
<th>Reward Mechanism</th>
</tr>
</thead>
<tbody>
<tr>
<td><b>Infinite v/s Finite</b><ul><li>Finite - discretized. Choose an arm from a 3-arm bandit</li><li>Infinite - continuous i.e. choose from an interval between 0 and 1</li></ul></td>
<td>Will selecting an action reveal any information about an action that you did not play? If yes, the problem has structure! Otherwise, you have an unstructured bandit problem. <ul><li><b>Unstructured</b>: This just means that if I have a two-arm bandit, I cannot learn anything about arm two by playing arm one. I might know that they’re both generating rewards by Gaussian distributions, but that doesn’t give me any connection between the arms.</li><li><b>Structured</b>: A structured bandit leaks information about other arms. Example: We could have a two-arm bandit where both arms generate rewards with a Bernoulli distribution that have a single parameter. What I mean is say arm one is parameterized by <span class="math">\(\theta\)</span> and arm two is parameterized by <span class="math">\(1-\theta\)</span>. There’s only one parameter to learn, underlying the whole problem, and by playing the first arm, I gain information about the other arm! <br>Covariance matrix of the arm distributions -- <span class="math">\(cov(arms) \ne \mathbb{I}\)</span><br> <strong>Linear bandit model is structured</strong></li></ul></td>
<td><strong>Stochastic</strong>:Under the stochastic reward generation setting, each action corresponds to an IID reward. That is, each action has an underlying distribution that it samples from when an action is selected. <strong><em>It never changes</em></strong>, therefore the learner simply needs to explore the arm until it can properly bound the shape of the reward generating distribution.</td>
</tr>
<tr>
<td><strong>Single action v/s Combinatorial action</strong><ul><li>Selection of 1 action</li><li>Combinatorial - Select a Vector of actions</li></ul></td>
<td><strong>Contextual Bandit</strong> - External Information can be utilized about the <span class="math">\(\mathcal{E}\)</span> <br> <em>Ex</em> - Say you have a two-arm bandit, but the arms perform differently on different days of the week (but the same on identical days of the week). If you attack this problem with a normal bandit algorithm, the rewards will appear highly non-stationary and will harm performance. However, if you use the context (the days of the week) in the action decision process then your learner should perform much better!</td>
<td><strong>Stationary v/s Non-stationary</strong><ul><li><strong>Stationary</strong> - Reward distributions don't shift</li><li><strong>Non-Stationary</strong> - Reward distributions can shift and come at a cost to the algorithm</li></ul></td>
</tr>
<tr>
<td><b>Single arm v/s <span class="math">\(k\)</span>-armed</b>: A bandit environment (<span class="math">\(\mathcal{E}\)</span>) can have a single-arm yielding some reward or multi-armed (<span class="math">\(k\)</span> armed) setting.</td>
<td><strong>Reward Vectors</strong> Can be sampled from a <em>Gaussian distribution</em> or a <em>Bernouilli distribution</em></td>
<td><strong>Adversarial Bandits</strong> <ul><li>Adversary selects the worst-case rewards given knowledge of the learning agent's policy</li><li>In such cases, randomization becomes of key-importance so that adversary cannot predict our next move</li><ul></td>
</tr>
</tbody>
</table>
<hr>
<h6>Table of Algorithms - Frequentist Regret</h6>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Setting</th>
<th>Input</th>
<th>Environment</th>
<th>Regret</th>
</tr>
</thead>
<tbody>
<tr>
<td>ETC - Explore-then-Commit</td>
<td><ul><li> <span class="math">\(k\)</span> arms</li><li>Every arm has a probability distribution - <span class="math">\(P_{a_i}\)</span> + 1-sub-gaussian </li> <li><span class="math">\(m=f(\Delta,n)\)</span> </li></ul></td>
<td><ul><li><span class="math">\(\Delta_i\)</span> = sub-optimality gap between <span class="math">\(arm_1\)</span> and <span class="math">\(arm_i\)</span> = <span class="math">\(\mu_1 - \mu_i\)</span></li><li><span class="math">\(n\)</span> - time-horizon</li></ul></td>
<td><ul><li> Unstructured <span class="math">\(\mathcal{E}\)</span></li><li> Stochastic Stationary</li> <ul></td>
<td>For large n <br><span class="math">\(R_n \le \frac{4}{\Delta} ln(n) + C\)</span> <br><br> With doubling trick, no need for knowlede of n and <span class="math">\(R_n \le \frac{4}{\Delta ln(2)} {(ln(n))}^2 + C_1 ln(n)\)</span></td>
</tr>
<tr>
<td><span class="math">\(\mathcal{E}\)</span> - Greedy</td>
<td>Same as above except, there's no <span class="math">\(m\)</span></td>
<td>Same as above</td>
<td><ul><li> Unstructured <span class="math">\(\mathcal{E}\)</span></li><li> Stochastic Stationary</li> <ul></td>
<td>For large n <br><span class="math">\(R_n \le \frac{1}{\Delta} ln(n) + C\)</span></td>
</tr>
<tr>
<td>Elimination</td>
<td><ul><li>Phased elimination of arms, using a hypothesis test</li></ul></td>
<td><ul><li> Only time-horizon - <span class="math">\(n\)</span> </li><li><span class="math">\(\Delta\)</span> is unknown</li></ul></td>
<td><ul><li> Unstructured <span class="math">\(\mathcal{E}\)</span></li><li> Stochastic Stationary</li> <ul></td>
<td>For large n <br><span class="math">\(R_n \le \frac{C_2}{\Delta} ln(n) + C_1\)</span></td>
</tr>
<tr>
<td>Upper-Confidence-Bound (<span class="math">\(\delta\)</span>)<br> Known time horizon</td>
<td><ul><li>Based on principle of <strong>optimism in the face of uncertainty</strong></li><li>It creates an upper confidence bound, which is an overestimation of the unknown mean</li></ul></td>
<td><ul><li> Only time-horizon - <span class="math">\(n\)</span> </li><li><span class="math">\(\Delta\)</span> is unknown</li></ul></td>
<td><ul><li> Unstructured <span class="math">\(\mathcal{E}\)</span></li><li> Stochastic Stationary</li> <ul></td>
<td></td>
</tr>
<tr>
<td>Upper-Confidence-Bound - Arbitrary/Infinite Time Horizon</td>
<td>Same as above</td>
<td><ul><li> Time-horizon - <span class="math">\(n\)</span> is unknown and arbitrarily set</li><li><span class="math">\(\Delta\)</span> is unknown</li></ul></td>
<td><ul><li> Unstructured <span class="math">\(\mathcal{E}\)</span></li><li> Stochastic Stationary</li> <ul></td>
<td></td>
</tr>
<tr>
<td>KL - Upper-Confidence-Bound (<span class="math">\(\delta\)</span>)<br> Known time horizon</td>
<td><ul><li>Difference between UCB and KL-UCB is that Chernoff's bound is used to define the UCB in this version</li></ul></td>
<td>Same as UCB</td>
<td><ul><li> Unstructured <span class="math">\(\mathcal{E}\)</span></li><li> Stochastic Stationary</li> <ul></td>
<td></td>
</tr>
<tr>
<td>EXP3 - Exponential weight algo for Exploit and Explore</td>
<td><ul><li>Abandon all assumptions on the reward mechanisms i.e. <strong>adversarial</strong></li><li>Calculate a softmax sampling distribution using action-reward observed so far</li><li>Choose randomly from this sampling distribution</li></ul></td>
<td><ul><li> Learning rate calculated from time horizon( <span class="math">\(n\)</span>) <br> <span class="math">\(\eta = \sqrt{\frac{log(k)}{(nk)}}\)</span> </li></ul></td>
<td><ul><li> Unstructured <span class="math">\(\mathcal{E}\)</span></li><li>Adversarial</li><ul></td>
<td></td>
</tr>
<tr>
<td>LinUCB</td>
<td></td>
<td></td>
<td><ul><li><strong>Contextual</strong></li><li><strong>Linear</strong></li><li> Structured - <span class="math">\(\mathcal{E}\)</span></li><li>Stochastic Stationary</li> <ul></td>
<td></td>
</tr>
</tbody>
</table>
<hr>
<h6>Table of Algorithms - Bayesian Regret</h6>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Setting</th>
<th>Input</th>
<th>Environment</th>
</tr>
</thead>
<tbody>
<tr>
<td>Thompson Sampling</td>
<td><ul><li>Bayesian Bandit environment - (<span class="math">\(\mathcal{E},\mathfrak{B}(\mathcal{E}),Q,P\)</span>)</li></ul></td>
<td>Prior Bayesian, Conjugate  pairs</td>
<td><ul><li>We don't have a fixed environment and have a distribution of environments </li></ul></td>
</tr>
</tbody>
</table>
<hr>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://rishacha.github.io/tag/multi-armed-bandits.html">multi-armed-bandits</a>
    </p>
  </div>





</article>

    <footer>
<p>&copy;  </p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
  <span class="footer-separator">|</span>
  Switch to the <a href="javascript:void(0)" onclick="theme.switch(`dark`)">dark</a> | <a href="javascript:void(0)" onclick="theme.switch(`light`)">light</a> | <a href="javascript:void(0)" onclick="theme.switch(`browser`)">browser</a> theme
  <script id="dark-theme-script"
          src="https://rishacha.github.io/theme/dark-theme/dark-theme.min.js"
          data-enable-auto-detect-theme="True"
          data-default-theme="light"
          type="text/javascript">
  </script>
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Rishacha's Blog ",
  "url" : "https://rishacha.github.io",
  "image": "/media/rctest.jpg",
  "description": "Developer searching for excitement"
}
</script>


</body>
</html>